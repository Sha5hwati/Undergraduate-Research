{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mnist\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data\n",
    "train = mnist.train_images()\n",
    "label_train = mnist.train_labels()\n",
    "test = mnist.test_images()\n",
    "label_test = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the data for the decision tree\n",
    "nsamples, nx, ny = train.shape\n",
    "train = train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = test.shape\n",
    "test = test.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    if inner_tree.value[index].min() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "    # if there are shildren, visit them as well\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the decision tree of depth 10\n",
    "start = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 10, random_state = 1)\n",
    "dt.fit(train, label_train)\n",
    "prune_index(dt.tree_, 0, 1)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "pred = dt.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the metrics\n",
    "def metrics(cm, cls, size):\n",
    "    cm = np.array(cm)\n",
    "    tp = cm[cls][cls]\n",
    "    fp = sum(cm[x, cls] for x in range(10))-cm[cls][cls]\n",
    "    fn = sum(cm[cls, x] for x in range(10))-cm[cls][cls]\n",
    "    tn = size - tp - fp - fn\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    fmeasure = 2*(precision*recall)/(precision + recall)\n",
    "    accuracy = (tp + tn)/size\n",
    "    \n",
    "    return precision, recall, fmeasure, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " \n",
      "[[ 839    1    3    4    1   55   18    4   24   31]\n",
      " [   1 1043   43   15    3    9    3    1   17    0]\n",
      " [  12   62  760    8   19   20   52   21   57   21]\n",
      " [  16   16   53  714    8   63   12   12   93   23]\n",
      " [   4    9    5    1  705   23   20   28   42  145]\n",
      " [  29    9    7   60   13  630   45   47   33   19]\n",
      " [  27   20    9    1   19   34  766    3   26   53]\n",
      " [   2   37   50    2    9   10    1  830   28   59]\n",
      " [   6   25   28   25    8   46   62    5  712   57]\n",
      " [   6   12    4   25   29   37    9   37   60  790]]\n"
     ]
    }
   ],
   "source": [
    "# Rows: Actual\n",
    "# Cols: Predicted\n",
    "cm = confusion_matrix(label_test, pred)\n",
    "print(\"Confusion Matrix:\\n \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 0:  0.891    0.856   0.873     0.976\n"
     ]
    }
   ],
   "source": [
    "# Class 0\n",
    "precision0, recall0, f0, acc0 = metrics(cm, 0, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 0: \", round(precision0, 3), \"  \", round(recall0, 3), \\\n",
    "      \" \", round(f0, 3), \"   \", round(acc0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 1:  0.845    0.919   0.881     0.972\n"
     ]
    }
   ],
   "source": [
    "# Class 1\n",
    "precision1, recall1, f1, acc1 = metrics(cm, 1, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 1: \", round(precision1, 3), \"  \", round(recall1, 3), \\\n",
    "      \" \", round(f1, 3), \"   \", round(acc1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 2:  0.79    0.736   0.762     0.953\n"
     ]
    }
   ],
   "source": [
    "# Class 2\n",
    "precision2, recall2, f2, acc2 = metrics(cm, 2, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 2: \", round(precision2, 3), \"  \", round(recall2, 3), \\\n",
    "      \" \", round(f2, 3), \"   \", round(acc2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 3:  0.835    0.707   0.766     0.956\n"
     ]
    }
   ],
   "source": [
    "# Class 3\n",
    "precision3, recall3, f3, acc3 = metrics(cm, 3, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 3: \", round(precision3, 3), \"  \", round(recall3, 3), \\\n",
    "      \" \", round(f3, 3), \"   \", round(acc3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 4:  0.866    0.718   0.785     0.961\n"
     ]
    }
   ],
   "source": [
    "# Class 4\n",
    "precision4, recall4, f4, acc4 = metrics(cm, 4, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 4: \", round(precision4, 3), \"  \", round(recall4, 3), \\\n",
    "      \" \", round(f4, 3), \"   \", round(acc4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 5:  0.68    0.706   0.693     0.944\n"
     ]
    }
   ],
   "source": [
    "# Class 5\n",
    "precision5, recall5, f5, acc5 = metrics(cm, 5, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 5: \", round(precision5, 3), \"  \", round(recall5, 3), \\\n",
    "      \" \", round(f5, 3), \"   \", round(acc5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 6:  0.775    0.8   0.787     0.959\n"
     ]
    }
   ],
   "source": [
    "# Class 6\n",
    "precision6, recall6, f6, acc6 = metrics(cm, 6, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 6: \", round(precision6, 3), \"  \", round(recall6, 3), \\\n",
    "      \" \", round(f6, 3), \"   \", round(acc6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 7:  0.891    0.807   0.823     0.964\n"
     ]
    }
   ],
   "source": [
    "# Class 7\n",
    "precision7, recall7, f7, acc7 = metrics(cm, 7, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 7: \", round(precision0, 3), \"  \", round(recall7, 3), \\\n",
    "      \" \", round(f7, 3), \"   \", round(acc7,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 8:  0.652    0.731   0.689     0.936\n"
     ]
    }
   ],
   "source": [
    "# Class 8\n",
    "precision8, recall8, f8, acc8 = metrics(cm, 8, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 8: \", round(precision8, 3), \"  \", round(recall8, 3), \\\n",
    "      \" \", round(f8, 3), \"   \", round(acc8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Precision Recall F-measure Accuracy\n",
      "Class 9:  0.659    0.783   0.716     0.937\n"
     ]
    }
   ],
   "source": [
    "# Class 9\n",
    "precision9, recall9, f9, acc9 = metrics(cm, 9, len(test))\n",
    "print(\"        Precision Recall F-measure Accuracy\")\n",
    "print(\"Class 9: \", round(precision9, 3), \"  \", round(recall9, 3), \\\n",
    "      \" \", round(f9, 3), \"   \", round(acc9,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score:  0.7789\n"
     ]
    }
   ],
   "source": [
    "# number of instances classified correctly\n",
    "acc_score = accuracy_score(pred, label_test)\n",
    "print(\"Accuracy_score: \", round(acc_score, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.52419 secs\n"
     ]
    }
   ],
   "source": [
    "# training time\n",
    "print(\"Training Time: %s secs\" % round(end - start, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree visualization\n",
    "#dot_data = StringIO()\n",
    "#export_graphviz(dt, out_file=dot_data)\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
